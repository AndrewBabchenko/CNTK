{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNTK 204: Sequence to Sequence Networks with Text Data\n",
    "\n",
    "\n",
    "## Introduction and Background\n",
    "\n",
    "This hands-on tutorial will take you through both the basics of sequence-to-sequence networks, and how to implement them in the Microsoft Cognitive Toolkit. In particular, we will implement a sequence-to-sequence model with attention to perform grapheme to phoneme translation. We will start with some basic theory and then explain the data in more detail, and how you can download it.\n",
    "\n",
    "Andrej Karpathy has a [nice visualization](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) of five common paradigms of neural network architectures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://cntk.ai/jup/paradigms.jpg\" width=\"750\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Figure 1\n",
    "Image(url=\"http://cntk.ai/jup/paradigms.jpg\", width=750)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we are going to be talking about the fourth paradigm: many-to-many where the length of the output does not necessarily equal the length of the input, also known as sequence-to-sequence networks. The input is a sequence with a dynamic length, and the output is also a sequence with some dynamic length. It is the logical extension of the many-to-one paradigm in that previously we were predicting some category (which could easily be one of `V` words where `V` is an entire vocabulary) and now we want to predict a whole sequence of those categories.\n",
    "\n",
    "The applications of sequence-to-sequence networks are nearly limitless. It is a natural fit for machine translation (e.g. English input sequences, French output sequences); automatic text summarization (e.g. full document input sequence, summary output sequence); word to pronunciation models (e.g. character [grapheme] input sequence, pronunciation [phoneme] output sequence); and even parse tree generation (e.g. regular text input, flat parse tree output).\n",
    "\n",
    "## Basic theory\n",
    "\n",
    "A sequence-to-sequence model consists of two main pieces: (1) an encoder; and (2) a decoder. Both the encoder and the decoder are recurrent neural network (RNN) layers that can be implemented using a vanilla RNN, an LSTM, or GRU cells (here we will use LSTM). In the basic sequence-to-sequence model, the encoder processes the input sequence into a fixed representation that is fed into the decoder as a context. The decoder then uses some mechanism (discussed below) to decode the processed information into an output sequence. The decoder is a language model that is augmented with some \"strong context\" by the encoder, and so each symbol that it generates is fed back into the decoder for additional context (like a traditional LM). For an English to German translation task, the most basic setup might look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://cntk.ai/jup/s2s.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Figure 2\n",
    "Image(url=\"http://cntk.ai/jup/s2s.png\", width=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic sequence-to-sequence network passes the information from the encoder to the decoder by initializing the decoder RNN with the final hidden state of the encoder as its initial hidden state. The input is then a \"sequence start\" tag (`<s>` in the diagram above) which primes the decoder to start generating an output sequence. Then, whatever word (or note or image, etc.) it generates at that step is fed in as the input for the next step. The decoder keeps generating outputs until it hits the special \"end sequence\" tag (`</s>` above).\n",
    "\n",
    "A more complex and powerful version of the basic sequence-to-sequence network uses an attention model. While the above setup works well, it can start to break down when the input sequences get long. At each step, the hidden state `h` is getting updated with the most recent information, and therefore `h` might be getting \"diluted\" in information as it processes each token. Further, even with a relatively short sequence, the last token will always get the last say and therefore the thought vector will be somewhat biased/weighted towards that last word. To deal with this problem, we use an \"attention\" mechanism that allows the decoder to look not only at all of the hidden states from the input, but it also learns which hidden states, for each step in decoding, to put the most weight on. In this tutorial we will implement a sequence-to-sequence network that can be run either with or without attention enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"s2s2.png\" width=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Figure 3\n",
    "Image(url=\"s2s2.png\", width=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Attention` layer above takes the current value of the hidden state in the Decoder, all of the hidden states in the Encoder, and calculates an augmented version of the hidden state to use. More specifically, the contribution from the Encoder's hidden states will represent a weighted sum of all of its hidden states where the highest weight corresponds both to the biggest contribution to the augmented hidden state and to the hidden state that will be most important for the Decoder to consider when generating the next word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem: Grapheme-to-Phoneme Conversion\n",
    "\n",
    "The [grapheme](https://en.wikipedia.org/wiki/Grapheme) to [phoneme](https://en.wikipedia.org/wiki/Phoneme) problem is a translation task that takes the letters of a word as the input sequence (the graphemes are the smallest units of a writing system) and outputs the corresponding phonemes; that is, the units of sound that make up a language. In other words, the system aims to generate an unambigious representation of how to pronounce a given input word.\n",
    "\n",
    "### Example\n",
    "\n",
    "The graphemes or the letters are translated into corresponding phonemes: \n",
    "\n",
    "> **Grapheme** : **|** T **|** A **|** N **|** G **|** E **|** R **|**  \n",
    "**Phonemes** : **|** ~T **|** ~AE **|** ~NG **|** ~ER **|**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task and Model Structure\n",
    "\n",
    "As discussed above, the task we are interested in solving is creating a model that takes some sequence as an input, and generates an output sequence based on the contents of the input. The model's job is to learn the mapping from the input sequence to the output sequence that it will generate. The job of the encoder is to come up with a good representation of the input that the decoder can use to generate a good output. For both the encoder and the decoder, the LSTM does a good job at this.\n",
    "\n",
    "We will use the LSTM implementation from the CNTK Layers library. This implements the \"smarts\" of the LSTM and we can more or less think of it as a black box. What is important to understand, however, is that there are two pieces to think of when implementing an RNN: the recurrence, which is the unrolled network over a sequence, and the block, which is the piece of the network run for each element of the sequence. We only need to implement the recurrence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing CNTK and other useful libraries\n",
    "\n",
    "CNTK is a Python module that contains several submodules like `io`, `learner`, `graph`, etc. We make extensive use of numpy as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import os\n",
    "from cntk import Trainer, Axis\n",
    "from cntk.io import MinibatchSource, CTFDeserializer, StreamDef, StreamDefs, INFINITELY_REPEAT, FULL_DATA_SWEEP\n",
    "from cntk.learner import momentum_sgd, adam_sgd, momentum_as_time_constant_schedule, learning_rate_schedule, UnitType\n",
    "from cntk.ops import input_variable, cross_entropy_with_softmax, classification_error, sequence, past_value, future_value, \\\n",
    "                     element_select, alias, hardmax, placeholder_variable, combine, parameter, times, plus\n",
    "from cntk.ops.functions import CloneMethod, load_model, Function\n",
    "from cntk.initializer import glorot_uniform\n",
    "from cntk.utils import log_number_of_parameters, ProgressPrinter, debughelpers, one_hot\n",
    "from cntk.graph import plot\n",
    "from cntk.layers import *\n",
    "from cntk.layers.sequence import *\n",
    "from cntk.models.attention import *\n",
    "from cntk.layers.typing import *\n",
    "\n",
    "# Select the right target device when this notebook is being tested:\n",
    "if 'TEST_DEVICE' in os.environ:\n",
    "    import cntk\n",
    "    if os.environ['TEST_DEVICE'] == 'cpu':\n",
    "        cntk.device.set_default_device(cntk.device.cpu())\n",
    "    else:\n",
    "        cntk.device.set_default_device(cntk.device.gpu(0))\n",
    "\n",
    "# Because of randomization in training we set a fixed random seed to ensure repeatable outputs\n",
    "from _cntk_py import set_fixed_random_seed\n",
    "set_fixed_random_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the data\n",
    "\n",
    "In this tutorial we will use a lightly pre-processed version of the CMUDict (version 0.7b) dataset from http://www.speech.cs.cmu.edu/cgi-bin/cmudict. The CMUDict data is the Carnegie Mellon University Pronouncing Dictionary is an open-source machine-readable pronunciation dictionary for North American English. The data is in the CNTKTextFormatReader format. Here is an example sequence pair from the data, where the input sequence (S0) is in the left column, and the output sequence (S1) is on the right:\n",
    "\n",
    "```\n",
    "0\t|S0 3:1 |# <s>\t|S1 3:1 |# <s>\n",
    "0\t|S0 4:1 |# A\t|S1 32:1 |# ~AH\n",
    "0\t|S0 5:1 |# B\t|S1 36:1 |# ~B\n",
    "0\t|S0 4:1 |# A\t|S1 31:1 |# ~AE\n",
    "0\t|S0 7:1 |# D\t|S1 38:1 |# ~D\n",
    "0\t|S0 12:1 |# I\t|S1 47:1 |# ~IY\n",
    "0\t|S0 1:1 |# </s>\t|S1 1:1 |# </s>\n",
    "```\n",
    "\n",
    "The code below will download the required files (training, the single sequence above for validation, and a small vocab file) and put them in a local folder (the training file is ~34 MB, testing is ~4MB, and the validation file and vocab file are both less than 1KB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reusing locally cached:  ..\\Examples\\SequenceToSequence\\CMUDict\\Data\\tiny.ctf\n",
      "Reusing locally cached:  ..\\Examples\\SequenceToSequence\\CMUDict\\Data\\cmudict-0.7b.train-dev-20-21.ctf\n",
      "Reusing locally cached:  ..\\Examples\\SequenceToSequence\\CMUDict\\Data\\cmudict-0.7b.mapping\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def download(url, filename):\n",
    "    \"\"\" utility function to download a file \"\"\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(filename, \"wb\") as handle:\n",
    "        for data in response.iter_content():\n",
    "            handle.write(data)\n",
    "\n",
    "MODEL_DIR = \".\"\n",
    "DATA_DIR = os.path.join('..', 'Examples', 'SequenceToSequence', 'CMUDict', 'Data')\n",
    "# If above directory does not exist, just use current.\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    data_dir = '.'\n",
    "\n",
    "VALIDATION_DATA = os.path.join(DATA_DIR, 'tiny.ctf')\n",
    "TRAINING_DATA = os.path.join(DATA_DIR, 'cmudict-0.7b.train-dev-20-21.ctf')\n",
    "VOCAB_FILE = os.path.join(DATA_DIR, 'cmudict-0.7b.mapping')\n",
    "\n",
    "files = [VALIDATION_DATA, TRAINING_DATA, VOCAB_FILE]\n",
    "\n",
    "for file in files:\n",
    "    if os.path.exists(file):\n",
    "        print(\"Reusing locally cached: \", file)\n",
    "    else:\n",
    "        url = \"https://github.com/Microsoft/CNTK/blob/v2.0.beta12.0/Examples/SequenceToSequence/CMUDict/Data/%s?raw=true\"%file\n",
    "        print(\"Starting download:\", file)\n",
    "        download(url, file)\n",
    "        print(\"Download completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reader\n",
    "\n",
    "To efficiently collect our data, randomize it for training, and pass it to the network, we use the CNTKTextFormat reader. We will create a small function that will be called when training (or testing) that defines the names of the streams in our data, and how they are referred to in the raw training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Helper function to load the model vocabulary file\n",
    "def get_vocab(path):\n",
    "    # get the vocab for printing output sequences in plaintext\n",
    "    vocab = [w.strip() for w in open(path).readlines()]\n",
    "    i2w = { i:w for i,w in enumerate(vocab) }\n",
    "    w2i = { w:i for i,w in enumerate(vocab) }\n",
    "    \n",
    "    return (vocab, i2w, w2i)\n",
    "\n",
    "# Read vocabulary data and generate their corresponding indices\n",
    "vocab, i2w, w2i = get_vocab(vocab_file)\n",
    "\n",
    "def create_reader(path, is_training):\n",
    "    return MinibatchSource(CTFDeserializer(path, StreamDefs(\n",
    "        features = StreamDef(field='S0', shape=input_vocab_dim, is_sparse=True),\n",
    "        labels   = StreamDef(field='S1', shape=label_vocab_dim, is_sparse=True)\n",
    "    )), randomize = is_training, epoch_size = INFINITELY_REPEAT if is_training else FULL_DATA_SWEEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size is 69\n",
      "First 15 letters are:\n",
      "[\"'\", '</s>', '<s/>', '<s>', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K']\n",
      "\n",
      "Print dictionary with the vocabulary mapping:\n",
      "{0: \"'\", 1: '</s>', 2: '<s/>', 3: '<s>', 4: 'A', 5: 'B', 6: 'C', 7: 'D', 8: 'E', 9: 'F', 10: 'G', 11: 'H', 12: 'I', 13: 'J', 14: 'K', 15: 'L', 16: 'M', 17: 'N', 18: 'O', 19: 'P', 20: 'Q', 21: 'R', 22: 'S', 23: 'T', 24: 'U', 25: 'V', 26: 'W', 27: 'X', 28: 'Y', 29: 'Z', 30: '~AA', 31: '~AE', 32: '~AH', 33: '~AO', 34: '~AW', 35: '~AY', 36: '~B', 37: '~CH', 38: '~D', 39: '~DH', 40: '~EH', 41: '~ER', 42: '~EY', 43: '~F', 44: '~G', 45: '~HH', 46: '~IH', 47: '~IY', 48: '~JH', 49: '~K', 50: '~L', 51: '~M', 52: '~N', 53: '~NG', 54: '~OW', 55: '~OY', 56: '~P', 57: '~R', 58: '~S', 59: '~SH', 60: '~T', 61: '~TH', 62: '~UH', 63: '~UW', 64: '~V', 65: '~W', 66: '~Y', 67: '~Z', 68: '~ZH'}\n"
     ]
    }
   ],
   "source": [
    "# Print vocab and the correspoding mapping to the phonemes\n",
    "print(\"Vocabulary size is\", len(vocab))\n",
    "print(\"First 15 letters are:\")\n",
    "print(vocab[:15])\n",
    "print()\n",
    "print(\"Print dictionary with the vocabulary mapping:\")\n",
    "print(i2w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the above to create a reader for our training data. Let's create it now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_reader(path, is_training):\n",
    "    return MinibatchSource(CTFDeserializer(path, StreamDefs(\n",
    "        features = StreamDef(field='S0', shape=input_vocab_dim, is_sparse=True),\n",
    "        labels   = StreamDef(field='S1', shape=label_vocab_dim, is_sparse=True)\n",
    "    )), randomize = is_training, epoch_size = INFINITELY_REPEAT if is_training else FULL_DATA_SWEEP)\n",
    "\n",
    "# Train data reader\n",
    "train_reader = create_reader(TRAINING_DATA, True)\n",
    "\n",
    "# Validation data reader\n",
    "valid_reader = create_reader(VALIDATION_DATA, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's set our model hyperparameters..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a number of settings that control the complexity of our network, the shapes of our inputs, and other options such as whether we will use an embedding (and what size to use), and whether or not we will employ attention. We set them now as they will be made use of when we build the network graph in the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_vocab_dim  = 69\n",
    "label_vocab_dim  = 69\n",
    "hidden_dim = 512\n",
    "num_layers = 2\n",
    "attention_dim = 128\n",
    "attention_span = 20\n",
    "attention_axis = -3\n",
    "use_attention = True\n",
    "use_embedding = True\n",
    "embedding_dim = 200\n",
    "vocab = ([w.strip() for w in open(VOCAB_FILE).readlines()]) # all lines of VOCAB_FILE in a list\n",
    "length_increase = 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set two more parameters now: the symbols used to denote the start of a sequence (sometimes called 'BOS') and the end of a sequence (sometimes called 'EOS'). In this case, our sequence-start symbol is the tag $<s>$ and our sequence-end symbol is the end-tag $</s>$.\n",
    "\n",
    "Sequence start and end tags are important in sequence-to-sequence networks for two reasons. The sequence start tag is a \"primer\" for the decoder; in other words, because we are generating an output sequence and RNNs require some input, the sequence start token \"primes\" the decoder to cause it to emit its first generated token. The sequence end token is important because the decoder will learn to output this token when the sequence is finished. Otherwise the network wouldn't know how long of a sequence to generated. For the code below, we setup the sequence start symbol as a `Constant` so that it can later be passed to the Decoder LSTM as its `initial_state`. Further, we get the sequence end symbol's index so that the Decoder can use it to know when to stop generating tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_start = Constant(np.array([w=='<s>' for w in vocab], dtype=np.float32))\n",
    "sentence_end_index = vocab.index('</s>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: setup the input to the network\n",
    "\n",
    "### Dynamic axes in CNTK (Key concept)\n",
    "\n",
    "One of the important concepts in understanding CNTK is the idea of two types of axes:\n",
    "- **static axes**, which are the traditional axes of a variable's shape, and\n",
    "- **dynamic axes**, which have dimensions that are unknown until the variable is bound to real data at computation time.\n",
    "\n",
    "The dynamic axes are particularly important in the world of recurrent neural networks. Instead of having to decide a maximum sequence length ahead of time, padding your sequences to that size, and wasting computation, CNTK's dynamic axes allow for variable sequence lengths that are automatically packed in minibatches to be as efficient as possible.\n",
    "\n",
    "When setting up sequences, there are *two dynamic axes* that are important to consider. The first is the *batch axis*, which is the axis along which multiple sequences are batched. The second is the dynamic axis particular to that sequence. The latter is specific to a particular input because of variable sequence lengths in your data. For example, in sequence to sequence networks, we have two sequences: the **input sequence**, and the **output (or 'label') sequence**. One of the things that makes this type of network so powerful is that the length of the input sequence and the output sequence do not have to correspond to each other. Therefore, both the input sequence and the output sequence require their own unique dynamic axis.\n",
    "\n",
    "We first create the `inputAxis` for the input sequence and the `labelAxis` for the output sequence. We then define the inputs to the model by creating sequences over these two unique dynamic axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Source and target inputs to the model\n",
    "inputAxis = Axis('inputAxis')\n",
    "labelAxis = Axis('labelAxis')\n",
    "InputSequence = SequenceOver[inputAxis]\n",
    "LabelSequence = SequenceOver[labelAxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: define the network\n",
    "\n",
    "As discussed before, the sequence-to-sequence network is, at its most basic, an RNN (LSTM) encoder followed by an RNN (LSTM) decoder, and a dense output layer. We will implement both the Encoder and the Decoder using the CNTK Layers library. Both of these will be created as CNTK Functions. Our `create_model()` Python function creates both the `encode` and `decode` CNTK Functions. The `decode` function directly makes use of the `encode` function and the return value of `create_model()` is the CNTK Function `decode` itself.\n",
    "\n",
    "We start by passing the input through an embedding (learned as part of the training process). So that this function can be used in the `Sequential` block of the Encoder and the Decoder whether we want an embedding or not, we will use the `identity` function if the `use_embedding` parameter is `False`. We then declare the Encoder layers as follows:\n",
    "\n",
    "First, we pass the input through our `embed` function and then we stabilize it. This adds an additional scalar parameter to the learning that can help our network converge more quickly during training. Then, for each of the number of LSTM layers that we want in our encoder, except the final one, we set up an LSTM recurrence. The final recurrence will be a `Fold` if we are not using attention because we only pass the final hidden state to the decoder. If we are using attention, however, then we use another normal LSTM `Recurrence` that the Decoder will put its attention over later on.\n",
    "\n",
    "Below we see a diagram of how the layered version of the sequence-to-sequence network with attention works. As the code shows below, the output of each layer of the Encoder and Decoder is used as the input to the layer just above it. The Attention model focuses on the top layer of the Encoder and informs the first layer of the Decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"s2s3.png\" width=\"900\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Figure 4\n",
    "Image(url=\"s2s3.png\", width=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the decoder, we first define several sub-layers: the `Stabilizer` for the decoder input, the `Recurrence` blocks for each of the decoder's layers, the `Stabilizer` for the output of the stack of LSTMs, and the final `Dense` output layer. If we are using attention, then we also create an `AttentionModel` function `attention_model` which returns an augmented version of the decoder's hidden state with emphasis placed on the encoder hidden states that should be most used for the given step while generating the next output token.\n",
    "\n",
    "We then build the CNTK Function `decode`. The decorator `@Function` turns a regular Python function into a proper CNTK Function with the given arguments and return value. The Decoder works differently during training than it does during test time. During training, the history (i.e. input) to the Decoder `Recurrence` consists of the ground-truth labels. This means that while generating $y^{(t=2)}$, for example, the input will be $y^{(t=1)}$. During evaluation, or \"test time\", however, the input to the Decoder will be the actual output of the model. For a greedy decoder -- which we are implementing here -- that input is therefore the `hardmax` of the final `Dense` layer.\n",
    "\n",
    "The Decoder Function `decode` takes two arguments: (1) the `input` sequence; and (2) the Decoder `history`. First, it runs the `input` sequence through the Encoder function `encode` that we setup earlier. We then get the `history` and map it to its embedding if necessary. Then the embedded representation is stabilized before running it through the Decoder's `Recurrence`. For each layer of `Recurrence`, we run the embedded `history` (now represented as `r`) through the `Recurrence`'s LSTM. If we are not using attention, we run it through the `Recurrence` with its initial state set to the value of the final hidden state of the encoder (note that since we run the Encoder backwards when not using attention that the \"final\" hidden state is actually the first hidden state in chronological time). If we are using attention, however, then we calculate the auxiliary input `h_att` using our `attention_model` function and we splice that onto the input `x`. This augmented `x` is then used as input for the Decoder's `Recurrence`.\n",
    "\n",
    "Finally, we stabilize the output of the Decoder, put it through the final `Dense` layer `proj_out`, and label the output using the `Label` layer which allows for simple access to that layer later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create the s2s model\n",
    "def create_model(): # :: (history*, input*) -> logP(w)*\n",
    "    \n",
    "    # Embedding: (input*) --> embedded_input*\n",
    "    embed = Embedding(embedding_dim, name='embed') if use_embedding else identity\n",
    "    \n",
    "    # Encoder: (input*) --> (h0, c0)\n",
    "    # Create multiple layers of LSTMs by passing the output of the i-th layer\n",
    "    # to the (i+1)th layer as its input\n",
    "    # Note: We go_backwards for the plain model, but forward for the attention model.\n",
    "    with default_options(enable_self_stabilization=True, go_backwards=not use_attention):\n",
    "        LastRecurrence = Fold if not use_attention else Recurrence\n",
    "        encode = Sequential([\n",
    "            embed,\n",
    "            Stabilizer(),\n",
    "            For(range(num_layers-1), lambda:\n",
    "                Recurrence(LSTM(hidden_dim))),\n",
    "            LastRecurrence(LSTM(hidden_dim), return_full_state=True),\n",
    "            (Label('encoded_h'), Label('encoded_c')),\n",
    "        ])\n",
    "\n",
    "    # Decoder: (history*, input*) --> unnormalized_word_logp*\n",
    "    # where history is one of these, delayed by 1 step and <s> prepended:\n",
    "    #  - training: labels\n",
    "    #  - testing:  its own output hardmax(z) (greedy decoder)\n",
    "    with default_options(enable_self_stabilization=True):\n",
    "        # sub-layers\n",
    "        stab_in = Stabilizer()\n",
    "        rec_blocks = [LSTM(hidden_dim) for i in range(num_layers)]\n",
    "        stab_out = Stabilizer()\n",
    "        proj_out = Dense(label_vocab_dim, name='out_proj')\n",
    "        # attention model\n",
    "        if use_attention: # maps a decoder hidden state and the entire encoder state into an augmented decoder state\n",
    "            attention_model = AttentionModel(attention_dim, attention_span, attention_axis, name='attention_model') # :: (h_enc*, h_dec) -> (h_dec augmented)\n",
    "        # layer function\n",
    "        @Function\n",
    "        def decode(history, input):\n",
    "            encoded_input = encode(input)\n",
    "            r = history\n",
    "            r = embed(r)\n",
    "            r = stab_in(r)\n",
    "            for i in range(num_layers):\n",
    "                rec_block = rec_blocks[i]   # LSTM(hidden_dim)  # :: (dh, dc, x) -> (h, c)\n",
    "                if use_attention:\n",
    "                    if i == 0:\n",
    "                        @Function\n",
    "                        def lstm_with_attention(dh, dc, x):\n",
    "                            h_att = attention_model(encoded_input.outputs[0], dh)\n",
    "                            x = splice(x, h_att)\n",
    "                            return rec_block(dh, dc, x)\n",
    "                        r = Recurrence(lstm_with_attention)(r)\n",
    "                    else:\n",
    "                        r = Recurrence(rec_block)(r)\n",
    "                else:\n",
    "                    # unlike Recurrence(), the RecurrenceFrom() layer takes the initial hidden state as a data input\n",
    "                    r = RecurrenceFrom(rec_block)(*(encoded_input.outputs + (r,))) # :: h0, c0, r -> h                    \n",
    "            r = stab_out(r)\n",
    "            r = proj_out(r)\n",
    "            r = Label('out_proj_out')(r)\n",
    "            return r\n",
    "\n",
    "    return decode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network that we defined above can be thought of as an \"abstract\" model that must first be wrapped to be used. In this case, we will use it first to create a \"training\" version of the model (where the history for the Decoder will be the ground-truth labels), and then we will use it to create a greedy \"decoding\" version of the model where the history for the Decoder will be the `hardmax` output of the network. Let's set up these model wrappers next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Before starting training, we will define the training wrapper, the greedy decoding wrapper, and the criterion function used for training the model. Let's start with the training wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_model_train(s2smodel):\n",
    "    # model used in training (history is known from labels)\n",
    "    # note: the labels must NOT contain the initial <s>\n",
    "    @Function\n",
    "    def model_train(input, labels): # (input*, labels*) --> (word_logp*)\n",
    "\n",
    "        # The input to the decoder always starts with the special label sequence start token.\n",
    "        # Then, use the previous value of the label sequence (for training) or the output (for execution).\n",
    "        past_labels = Delay(initial_state=sentence_start)(labels)\n",
    "        return s2smodel(past_labels, input)\n",
    "    return model_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we create the CNTK Function `model_train` again using the `@Function` decorator. This function takes the input sequence `input` and the output sequence `labels` as arguments. The `past_labels` are setup as the `history` for the model we created earlier by using the `Delay` layer. This will return the previous time-step value for the input `labels` with an `initial_state` of `sentence_start`. Therefore, if we give the labels `['a', 'b', 'c']`, then `past_labels` will contain `['<s>', 'a', 'b', 'c']` and then return our abstract base model called with the history `past_labels` and the input `input`.\n",
    "\n",
    "Let's go ahead and create the greedy decoding model wrapper now as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_model_greedy(s2smodel):\n",
    "    # model used in (greedy) decoding (history is decoder's own output)\n",
    "    @Function\n",
    "    @Signature(InputSequence[Tensor[input_vocab_dim]])\n",
    "    def model_greedy(input): # (input*) --> (word_sequence*)\n",
    "\n",
    "        # Decoding is an unfold() operation starting from sentence_start.\n",
    "        # We must transform s2smodel (history*, input* -> word_logp*) into a generator (history* -> output*)\n",
    "        # which holds 'input' in its closure.\n",
    "        unfold = UnfoldFrom(lambda history: s2smodel(history, input) >> hardmax,\n",
    "                            until_predicate=lambda w: w[...,sentence_end_index],  # stop once sentence_end_index was max-scoring output\n",
    "                            length_increase=length_increase, initial_state=sentence_start)\n",
    "        \n",
    "        return unfold(dynamic_axes_like=input)\n",
    "    return model_greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we create a new CNTK Function `model_greedy` which this time only takes a single argument. This is of course because when using the model at test time we don't have any labels -- it is the model's job to create them for us! In this case, we use the `UnfoldFrom` layer which runs the base model with the current `history` and funnels it into the `hardmax`. The `hardmax`'s output then becomes part of the `history` and we keep unfolding the `Recurrence` until the `sentence_end_index` has been reached. The maximum length of the output sequence (the maximum unfolding of the Decoder) is determined by a multiplier passed to `length_increase`. In this case we set `length_increase` to `1.5` above so the maximum length of each output sequence is 1.5x its input.\n",
    "\n",
    "The last thing we will do before setting up the training loop is define the function that will create the criterion function for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_criterion_function(model):\n",
    "    @Function\n",
    "    @Signature(input = InputSequence[Tensor[input_vocab_dim]], labels = LabelSequence[Tensor[label_vocab_dim]])\n",
    "    def criterion(input, labels):\n",
    "        # criterion function must drop the <s> from the labels\n",
    "        postprocessed_labels = sequence.slice(labels, 1, 0) # <s> A B C </s> --> A B C </s>\n",
    "        z = model(input, postprocessed_labels)\n",
    "        ce   = cross_entropy_with_softmax(z, postprocessed_labels)\n",
    "        errs = classification_error      (z, postprocessed_labels)\n",
    "        return (ce, errs)\n",
    "\n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we create the criterion function which drops the sequence-start symbol from our labels for us, runs the model with the given `input` and `labels`, and uses the output to compare to our ground truth. We use the loss function `cross_entropy_with_softmax` and get the `classification_error` which gives us the percent-error per-word of our generation accuracy. The CNTK Function `criterion` returns these values as a tuple and the Python function `create_criterion_function(model)` returns that CNTK Function.\n",
    "\n",
    "Now let's move on to creating the training loop..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(train_reader, valid_reader, vocab, i2w, s2smodel, max_epochs, epoch_size):\n",
    "\n",
    "    # create the training wrapper for the s2smodel, as well as the criterion function\n",
    "    model_train = create_model_train(s2smodel)\n",
    "    criterion = create_criterion_function(model_train)\n",
    "\n",
    "    # also wire in a greedy decoder so that we can properly log progress on a validation example\n",
    "    # This is not used for the actual training process.\n",
    "    model_greedy = create_model_greedy(s2smodel)\n",
    "\n",
    "    # Instantiate the trainer object to drive the model training\n",
    "    minibatch_size = 72\n",
    "    lr = 0.001 if use_attention else 0.005\n",
    "    learner = adam_sgd(model_train.parameters,\n",
    "                       lr       = learning_rate_schedule([lr]*2+[lr/2]*3+[lr/4], UnitType.sample, epoch_size),\n",
    "                       momentum = momentum_as_time_constant_schedule(1100),\n",
    "                       gradient_clipping_threshold_per_sample=2.3,\n",
    "                       gradient_clipping_with_truncation=True)\n",
    "    trainer = Trainer(None, criterion, learner)\n",
    "\n",
    "    # Get minibatches of sequences to train with and perform model training\n",
    "    total_samples = 0\n",
    "    mbs = 0\n",
    "    eval_freq = 100\n",
    "\n",
    "    # print out some useful training information\n",
    "    log_number_of_parameters(model_train) ; print()\n",
    "    progress_printer = ProgressPrinter(freq=30, tag='Training')    \n",
    "\n",
    "    # a hack to allow us to print sparse vectors\n",
    "    sparse_to_dense = create_sparse_to_dense(input_vocab_dim)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        while total_samples < (epoch+1) * epoch_size:\n",
    "            # get next minibatch of training data\n",
    "            mb_train = train_reader.next_minibatch(minibatch_size)\n",
    "            \n",
    "            # do the training\n",
    "            trainer.train_minibatch({criterion.arguments[0]: mb_train[train_reader.streams.features], \n",
    "                                     criterion.arguments[1]: mb_train[train_reader.streams.labels]})\n",
    "\n",
    "            progress_printer.update_with_trainer(trainer, with_metric=True) # log progress\n",
    "\n",
    "            # every N MBs evaluate on a test sequence to visually show how we're doing\n",
    "            if mbs % eval_freq == 0:\n",
    "                mb_valid = valid_reader.next_minibatch(1)\n",
    "\n",
    "                # run an eval on the decoder output model (i.e. don't use the groundtruth)\n",
    "                e = model_greedy(mb_valid[valid_reader.streams.features])\n",
    "                print(format_sequences(sparse_to_dense(mb_valid[valid_reader.streams.features]), i2w))\n",
    "                print(\"->\")\n",
    "                print(format_sequences(e, i2w))\n",
    "\n",
    "                # visualizing attention window\n",
    "                if use_attention:\n",
    "                    debug_attention(model_greedy, mb_valid[valid_reader.streams.features])\n",
    "\n",
    "            total_samples += mb_train[train_reader.streams.labels].num_samples\n",
    "            mbs += 1\n",
    "\n",
    "        # log a summary of the stats for the epoch\n",
    "        progress_printer.epoch_summary(with_metric=True)\n",
    "\n",
    "    # done: save the final model\n",
    "    model_path = \"model_%d.cmf\" % epoch\n",
    "    print(\"Saving final model to '%s'\" % model_path)\n",
    "    s2smodel.save(model_path)\n",
    "    print(\"%d epochs complete.\" % max_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above function, we created one version of the model for training (plus its associated criterion function) and one version of the model for evaluation. Normally this latter version would not be required but here we have done it so that we can periodically sample from the non-training model to visually understand how our model is converging by seeing the kinds of sequences that it generates as the training progresses.\n",
    "\n",
    "We then setup some standard variables required for the training loop. We set the `minibatch_size` (which refers to the total number of elements -- NOT sequences -- in a minibatch), the initial learning rate `lr`, we initialize a `learner` using the `adam_sgd` algorithm and a `learning_rate_schedule` that slowly reduces our learning rate. We make use of gradient clipping to help control exploding gradients, and we finally create our `Trainer` object `trainer`.\n",
    "\n",
    "We make use of CNTK's `ProgressPrinter` class which takes care of calculating average metrics per minibatch/epoch and we set it to update every 30 minibatches. And finally, before starting the training loop, we initialize a function called `sparse_to_dense` which we use to properly print out the input sequence data that we use for validation because it is sparse. That function is defined just below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dummy for printing the input sequence below. Currently needed because input is sparse.\n",
    "def create_sparse_to_dense(input_vocab_dim):\n",
    "    I = Constant(np.eye(input_vocab_dim))\n",
    "    @Function\n",
    "    @Signature(InputSequence[SparseTensor[input_vocab_dim]])\n",
    "    def no_op(input):\n",
    "        return times(input, I)\n",
    "    return no_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the training loop, we proceed much like many other CNTK networks. We request the next bunch of minibatch data, we perform our training, and we print our progress to the screen using the `progress_printer`. Where we diverge from the norm, however, is where we run an evaluation using our `model_greedy` version of the network and run a single sequence, \"ABADI\" through to see what the network is currently predicting.\n",
    "\n",
    "Another difference in the training loop is the optional attention window visualization. Calling the function `debug_attention` shows the weight that the Decoder put on each of the Encoder's hidden states for each of the output tokens that it generated. This function, along with the `format_sequences` function required to print the input/output sequences to the screen, are given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a vocab and tensor, print the output\n",
    "def format_sequences(sequences, i2w):\n",
    "    return [\" \".join([i2w[np.argmax(w)] for w in s]) for s in sequences]\n",
    "\n",
    "# to help debug the attention window\n",
    "def debug_attention(model, input):\n",
    "    q = combine([model, model.attention_model.attention_weights])\n",
    "    #words, p = q(input) # Python 3\n",
    "    words_p = q(input)\n",
    "    words = words_p[0]\n",
    "    p     = words_p[1]\n",
    "    len = words.shape[attention_axis-1]\n",
    "    span = 7 #attention_span  #7 # test sentence is 7 tokens long\n",
    "    p_sq = np.squeeze(p[0,:len,:span,0,:]) # (batch, len, attention_span, 1, vector_dim)\n",
    "    opts = np.get_printoptions()\n",
    "    np.set_printoptions(precision=5)\n",
    "    print(p_sq)\n",
    "    np.set_printoptions(**opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try training our network for a small part of an epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 7561400 parameters in 29 parameter tensors.\n",
      "\n",
      "['<s> A B A D I </s>']\n",
      "->\n",
      "['F ~D ~D ~D ~TH ~T ~T ~T ~DH Q ~N']\n",
      "[[ 0.14343  0.14279  0.14275  0.1431   0.1427   0.14281  0.14242]\n",
      " [ 0.14343  0.14279  0.14275  0.1431   0.1427   0.14281  0.14242]\n",
      " [ 0.14343  0.14279  0.14275  0.1431   0.1427   0.14281  0.14242]\n",
      " [ 0.14343  0.14279  0.14275  0.1431   0.1427   0.14281  0.14242]\n",
      " [ 0.14343  0.14279  0.14275  0.1431   0.1427   0.14281  0.14242]\n",
      " [ 0.14343  0.14279  0.14275  0.1431   0.1427   0.14281  0.14242]\n",
      " [ 0.14343  0.14279  0.14275  0.1431   0.1427   0.14281  0.14242]\n",
      " [ 0.14343  0.14279  0.14275  0.1431   0.1427   0.14281  0.14242]\n",
      " [ 0.14343  0.14279  0.14275  0.1431   0.1427   0.14281  0.14242]\n",
      " [ 0.14343  0.14279  0.14275  0.1431   0.1427   0.14281  0.14242]\n",
      " [ 0.14343  0.14279  0.14275  0.1431   0.1427   0.14281  0.14242]]\n",
      " Minibatch[   1-  30]: loss = 4.154019 * 1541, metric = 87.80% * 1541;\n",
      " Minibatch[  31-  60]: loss = 3.678370 * 1578, metric = 86.12% * 1578;\n",
      " Minibatch[  61-  90]: loss = 3.353857 * 1578, metric = 89.16% * 1578;\n",
      "['<s> A B A D I </s>']\n",
      "->\n",
      "['~AH ~AH ~AH </s>']\n",
      "[[ 0.14336  0.14273  0.1427   0.14309  0.14273  0.14287  0.14253]\n",
      " [ 0.14336  0.14273  0.1427   0.14309  0.14273  0.14287  0.14253]\n",
      " [ 0.14336  0.14273  0.1427   0.14309  0.14273  0.14287  0.14253]\n",
      " [ 0.14336  0.14273  0.1427   0.14309  0.14273  0.14287  0.14253]]\n",
      " Minibatch[  91- 120]: loss = 3.232332 * 1539, metric = 84.60% * 1539;\n",
      " Minibatch[ 121- 150]: loss = 3.190510 * 1572, metric = 83.46% * 1572;\n",
      " Minibatch[ 151- 180]: loss = 3.210020 * 1585, metric = 86.37% * 1585;\n",
      "['<s> A B A D I </s>']\n",
      "->\n",
      "['~R ~R ~N ~N </s>']\n",
      "[[ 0.14349  0.14283  0.14277  0.14312  0.1427   0.14276  0.14233]\n",
      " [ 0.14349  0.14283  0.14277  0.14312  0.1427   0.14276  0.14233]\n",
      " [ 0.14349  0.14283  0.14277  0.14312  0.1427   0.14276  0.14233]\n",
      " [ 0.14349  0.14283  0.14277  0.14312  0.1427   0.14276  0.14233]\n",
      " [ 0.14349  0.14283  0.14277  0.14312  0.1427   0.14276  0.14233]]\n",
      " Minibatch[ 181- 210]: loss = 3.135488 * 1542, metric = 84.05% * 1542;\n",
      " Minibatch[ 211- 240]: loss = 3.165797 * 1606, metric = 83.56% * 1606;\n",
      " Minibatch[ 241- 270]: loss = 3.164625 * 1580, metric = 83.67% * 1580;\n",
      " Minibatch[ 271- 300]: loss = 3.130015 * 1542, metric = 82.10% * 1542;\n",
      "['<s> A B A D I </s>']\n",
      "->\n",
      "['~R ~R ~R ~R ~T </s>']\n",
      "[[ 0.14366  0.14298  0.14286  0.14316  0.14265  0.14262  0.14206]\n",
      " [ 0.14366  0.14298  0.14286  0.14316  0.14265  0.14262  0.14206]\n",
      " [ 0.14366  0.14298  0.14286  0.14316  0.14265  0.14262  0.14206]\n",
      " [ 0.14366  0.14298  0.14286  0.14316  0.14266  0.14262  0.14206]\n",
      " [ 0.14366  0.14298  0.14286  0.14316  0.14266  0.14262  0.14206]\n",
      " [ 0.14366  0.14298  0.14286  0.14316  0.14266  0.14262  0.14206]]\n",
      " Minibatch[ 301- 330]: loss = 3.127071 * 1553, metric = 83.90% * 1553;\n",
      " Minibatch[ 331- 360]: loss = 3.109696 * 1587, metric = 82.17% * 1587;\n",
      " Minibatch[ 361- 390]: loss = 3.103379 * 1536, metric = 82.49% * 1536;\n",
      "['<s> A B A D I </s>']\n",
      "->\n",
      "['~R ~R ~AH ~N </s>']\n",
      "[[ 0.14378  0.14308  0.14291  0.14317  0.1426   0.14252  0.14195]\n",
      " [ 0.14378  0.14308  0.14291  0.14316  0.1426   0.14252  0.14196]\n",
      " [ 0.14377  0.14308  0.14291  0.14316  0.1426   0.14252  0.14196]\n",
      " [ 0.14377  0.14307  0.14291  0.14316  0.1426   0.14252  0.14196]\n",
      " [ 0.14377  0.14308  0.14291  0.14316  0.1426   0.14252  0.14196]]\n",
      " Minibatch[ 391- 420]: loss = 3.115195 * 1574, metric = 81.77% * 1574;\n",
      " Minibatch[ 421- 450]: loss = 3.081090 * 1574, metric = 82.47% * 1574;\n",
      " Minibatch[ 451- 480]: loss = 3.048026 * 1574, metric = 80.50% * 1574;\n",
      "['<s> A B A D I </s>']\n",
      "->\n",
      "['~K ~AE ~L ~AH ~L </s>']\n",
      "[[ 0.1441   0.14336  0.14308  0.14323  0.14251  0.14225  0.14146]\n",
      " [ 0.1441   0.14336  0.14308  0.14322  0.14251  0.14226  0.14147]\n",
      " [ 0.14411  0.14336  0.14308  0.14322  0.14251  0.14225  0.14146]\n",
      " [ 0.14411  0.14336  0.14308  0.14322  0.14251  0.14225  0.14146]\n",
      " [ 0.1441   0.14336  0.14308  0.14322  0.14251  0.14226  0.14147]\n",
      " [ 0.14411  0.14336  0.14308  0.14322  0.14251  0.14226  0.14146]]\n",
      " Minibatch[ 481- 510]: loss = 3.017086 * 1539, metric = 80.12% * 1539;\n",
      " Minibatch[ 511- 540]: loss = 2.979709 * 1521, metric = 79.29% * 1521;\n",
      " Minibatch[ 541- 570]: loss = 2.868699 * 1552, metric = 77.32% * 1552;\n",
      " Minibatch[ 571- 600]: loss = 2.838428 * 1570, metric = 77.45% * 1570;\n",
      "['<s> A B A D I </s>']\n",
      "->\n",
      "['~K ~AA ~R ~AH ~N </s>']\n",
      "[[ 0.14446  0.14366  0.14326  0.1433   0.14242  0.14196  0.14092]\n",
      " [ 0.14446  0.14366  0.14326  0.14329  0.14243  0.14197  0.14095]\n",
      " [ 0.14448  0.14368  0.14327  0.1433   0.14242  0.14195  0.14089]\n",
      " [ 0.14444  0.14364  0.14326  0.14329  0.14243  0.14198  0.14096]\n",
      " [ 0.14446  0.14366  0.14327  0.1433   0.14243  0.14196  0.14092]\n",
      " [ 0.14442  0.14364  0.14325  0.14328  0.14243  0.142    0.14097]]\n",
      " Minibatch[ 601- 630]: loss = 2.856439 * 1595, metric = 80.00% * 1595;\n",
      " Minibatch[ 631- 660]: loss = 2.767322 * 1582, metric = 77.62% * 1582;\n",
      " Minibatch[ 661- 690]: loss = 2.691303 * 1582, metric = 77.05% * 1582;\n",
      "['<s> A B A D I </s>']\n",
      "->\n",
      "['~R ~EH ~N ~AH ~N </s>']\n",
      "[[ 0.14474  0.14389  0.14337  0.14332  0.14232  0.14174  0.14062]\n",
      " [ 0.14473  0.14388  0.14336  0.1433   0.14232  0.14175  0.14066]\n",
      " [ 0.14475  0.1439   0.14336  0.14331  0.14232  0.14174  0.14062]\n",
      " [ 0.14469  0.14387  0.14337  0.14331  0.14232  0.14177  0.14066]\n",
      " [ 0.14471  0.14387  0.14336  0.14331  0.14232  0.14176  0.14067]\n",
      " [ 0.14467  0.14386  0.14336  0.1433   0.14233  0.14179  0.14069]]\n",
      " Minibatch[ 691- 720]: loss = 2.716964 * 1577, metric = 76.79% * 1577;\n",
      " Minibatch[ 721- 750]: loss = 2.776740 * 1556, metric = 76.48% * 1556;\n",
      " Minibatch[ 751- 780]: loss = 2.666363 * 1574, metric = 75.73% * 1574;\n",
      "['<s> A B A D I </s>']\n",
      "->\n",
      "['~K ~AA ~R ~AH ~N </s>']\n",
      "[[ 0.14493  0.14404  0.14343  0.14333  0.14224  0.1416   0.14044]\n",
      " [ 0.14494  0.14404  0.14341  0.14331  0.14224  0.14159  0.14046]\n",
      " [ 0.14497  0.14408  0.14345  0.14333  0.14222  0.14156  0.14038]\n",
      " [ 0.14488  0.14401  0.1434   0.14329  0.14224  0.14164  0.14053]\n",
      " [ 0.14489  0.14401  0.14343  0.14334  0.14226  0.14162  0.14045]\n",
      " [ 0.14486  0.14401  0.14341  0.14331  0.14225  0.14165  0.1405 ]]\n",
      " Minibatch[ 781- 810]: loss = 2.716481 * 1575, metric = 77.71% * 1575;\n",
      " Minibatch[ 811- 840]: loss = 2.647995 * 1595, metric = 74.61% * 1595;\n",
      "Finished Epoch[1 of 300]: [Training] loss = 3.052572 * 44032, metric = 81.21% * 44032 94.360s (466.6 samples/s);\n",
      "Saving final model to 'model_0.cmf'\n",
      "1 epochs complete.\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "train(train_reader, valid_reader, vocab, i2w, model, max_epochs=1, epoch_size=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, while the loss has come down quite a ways, the output sequence is still quite a ways off from what we expect. Uncomment the code below to run for a full epoch (notice that we switch the `epoch_size` parameter to the actual size of the training data) and by the end of the first epoch you will already see a very good grapheme-to-phoneme translation model running!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Uncomment the line below to train the model for a full epoch\n",
    "#train(train_reader, valid_reader, vocab, i2w, model, max_epochs=1, epoch_size=908241)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the network\n",
    "\n",
    "Now that we've trained a sequence-to-sequence network for graphme-to-phoneme translation, there are two important things we should do with it. First, we should test its accuracy on a held-out test set. Then, we should try it out in an interactive environment so that we can put in our own input sequences and see what the model predicts. Let's start by determining the test string error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
